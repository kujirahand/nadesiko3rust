//! è»¸è§£æå™¨

use crate::prepare;
use crate::strcur::StrCur;
use crate::kanautils;
use crate::josi_list;
use crate::reserved_words;
use crate::token::*;

/// æ–‡å­—åˆ—ã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«åŒºåˆ‡ã‚‹
pub fn tokenize(src: &str) -> Vec<Token> {
    // æ™®é€šã«ãƒˆãƒ¼ã‚¯ãƒ³ã«åŒºåˆ‡ã‚‹
    let tok: Vec<Token> = tokenize_src(src, 1);
    
    // åŠ©è©ã®ã€Œã¯ã€ã‚’ï¼ã«å±•é–‹ã™ã‚‹
    let mut last_is_eq = false;
    let mut result: Vec<Token> = vec![];
    for t in tok.into_iter() {
        if last_is_eq {
            last_is_eq = false;
            if t.kind == TokenKind::Comma { continue; } // ã€ŒAã¯,1ã€ã®ã‚ˆã†ãªå ´åˆã«ã‚«ãƒ³ãƒã‚’é£›ã°ã™
        }
        match &t.josi {
            Some(j) => {
                let line = t.line;
                if j.eq("ã¯") {
                    let mut t2 = t.clone();
                    t2.josi = None;
                    result.push(t2);
                    result.push(Token::new_char(TokenKind::Eq, '=', line));
                    last_is_eq = true;
                    continue;
                }    
            }
            None => {},
        }
        result.push(t);
    }
    result
}

pub fn tokenize_src(src: &str, line_begin: u32) -> Vec<Token> {
    let src = prepare::convert(src);
    let mut cur = StrCur::from(&src);
    let mut result: Vec<Token> = vec![];
    let mut line = line_begin;
    while cur.can_read() {
        if cur.skip_space() { continue; }
        let ch = cur.peek();
        match ch {
            '\n' => { result.push(read_lf(&mut cur, &mut line)); continue; },
            ';' => {
                if cur.eq_str(";;;") { // ã€Œã€‚ã€‚ã€‚ã€ã¨ã€Œã“ã“ã¾ã§ã€ã¯åŒã˜æ„å‘³
                    flag_push_n(TokenKind::BlockEnd, ';', &mut result, &mut cur, 3, line);
                    continue;
                }
                flag_push(TokenKind::Eol, &mut result, &mut cur, line); continue;
            },
            'ğŸ’§' => { flag_push(TokenKind::BlockEnd, &mut result, &mut cur, line); continue; }
            ',' => { flag_push(TokenKind::Comma, &mut result, &mut cur, line); continue; },
            '/' => { result.push(read_slash(&mut cur, &mut line)); continue; },
            'â€»' => { result.push(read_linecomment(&mut cur, &mut line)); continue; },
            '#' => { result.push(read_linecomment(&mut cur, &mut line)); continue; },
            // æ–‡å­—åˆ—è¨˜å·
            'ã€Œ' => { read_string(&mut result, &mut cur, &mut line, 'ã€', true); continue; }
            'ã€' => { read_string(&mut result, &mut cur, &mut line, 'ã€', false); continue; }
            '"' => { read_string(&mut result, &mut cur, &mut line, '"', true); continue; }
            '\'' => { read_string(&mut result, &mut cur, &mut line, '\'', false); continue; }
            //å„ç¨®ã‚«ãƒƒã‚³
            '(' => { flag_push(TokenKind::ParenL, &mut result, &mut cur, line); continue; },
            ')' => { flag_push_josi(TokenKind::ParenR, &mut result, &mut cur, line); continue; },
            '[' => { flag_push(TokenKind::BracketL, &mut result, &mut cur, line); continue; },
            ']' => { flag_push_josi(TokenKind::BracketR, &mut result, &mut cur, line); continue; },
            '{' => { flag_push(TokenKind::CurBracketL, &mut result, &mut cur, line); continue; },
            '}' => { flag_push_josi(TokenKind::CurBracketR, &mut result, &mut cur, line); continue; },
            // æ¼”ç®—å­
            '+' => { flag_push(TokenKind::Plus, &mut result, &mut cur, line); continue; },
            '-' => { flag_push(TokenKind::Minus, &mut result, &mut cur, line); continue; },
            '*' => { flag_push(TokenKind::Mul, &mut result, &mut cur, line); continue; },
            'Ã—' => { flag_push_n(TokenKind::Mul, '*', &mut result, &mut cur, 1, line); continue; },
            'Ã·' => { flag_push_n(TokenKind::Div, '/', &mut result, &mut cur, 1, line); continue; },
            '%' => { flag_push(TokenKind::Mod, &mut result, &mut cur, line); continue; },
            '^' => { flag_push(TokenKind::Pow, &mut result, &mut cur, line); continue; },
            '\\' => { flag_push(TokenKind::Flag, &mut result, &mut cur, line); continue; },
            '`' => { flag_push(TokenKind::Flag, &mut result, &mut cur, line); continue; },
            '~' => { flag_push(TokenKind::Flag, &mut result, &mut cur, line); continue; },
            'â‰§' => { flag_push(TokenKind::GtEq, &mut result, &mut cur, line); continue; },
            'â‰¦' => { flag_push(TokenKind::LtEq, &mut result, &mut cur, line); continue; },
            'â‰ ' => { flag_push(TokenKind::NotEq, &mut result, &mut cur, line); continue; },
            'çœŸ' => { flag_push_josi(TokenKind::True, &mut result, &mut cur, line); continue; },
            'å½' => { flag_push_josi(TokenKind::False, &mut result, &mut cur, line); continue; },
            '=' => {
                if cur.eq_str("==") { flag_push_n(TokenKind::Eq, '=', &mut result, &mut cur, 2, line); }
                else { flag_push_n(TokenKind::Eq, '=', &mut result, &mut cur, 1, line); }
                continue; 
            },
            '&' => { 
                if cur.eq_str("&&") { flag_push_n(TokenKind::And, '&', &mut result, &mut cur, 2, line); }
                else { flag_push_n(TokenKind::PlusStr, 'çµ', &mut result, &mut cur, 1, line); }
                continue; 
            },
            '|' => { 
                if cur.eq_str("||") { flag_push_n(TokenKind::Or, '|', &mut result, &mut cur, 2, line); }
                else { flag_push_n(TokenKind::Or, '|', &mut result, &mut cur, 1, line); }
                continue; 
            },
            '!' => {
                if cur.eq_str("!=") { flag_push_n(TokenKind::NotEq, 'â‰ ', &mut result, &mut cur, 2, line); }
                else { flag_push(TokenKind::Not, &mut result, &mut cur, line); }
                continue; 
            },
            '>' => {
                if cur.eq_str(">=") { flag_push_n(TokenKind::GtEq, 'â‰§', &mut result, &mut cur, 2, line); }
                else if cur.eq_str("><") { flag_push_n(TokenKind::NotEq, 'â‰ ', &mut result, &mut cur, 2, line); cur.next(); }
                else { flag_push(TokenKind::Gt, &mut result, &mut cur, line); }
                continue;
            },
            '<' => {
                if cur.eq_str("<=") { flag_push_n(TokenKind::LtEq, 'â‰¦', &mut result, &mut cur, 2, line); }
                else if cur.eq_str("<>") { flag_push_n(TokenKind::NotEq, 'â‰ ', &mut result, &mut cur, 2, line); }
                else { flag_push(TokenKind::Lt, &mut result, &mut cur, line); }
                continue;
            },
            'â—' => { flag_push(TokenKind::DefFunc, &mut result, &mut cur, line); continue; },
            // '!'..='.' => { flag_push(TokenKind::Flag, &mut result, &mut cur, line); continue; },
            // ':'..='@' => { flag_push(TokenKind::Flag, &mut result, &mut cur, line); continue; },
            // æ•°å€¤
            '0'..='9' => { result.push(read_number(&mut cur, &mut line)); continue; },
            // word
            'a'..='z' | 'A'..='Z' | '_' => { read_word(&mut result, &mut cur, &mut line); continue; }
            n if n > (0xE0 as char) => { read_word(&mut result, &mut cur, &mut line); continue; }
            _ => {} // pass
        }
        // pass
        println!("[å­—å¥è§£æã‚¨ãƒ©ãƒ¼]: æœªå®šç¾©ã®æ–‡å­—ã€{}ã€", ch);
        cur.next();
    }
    result
}

// 1æ–‡å­—ã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦è¿½åŠ ã™ã‚‹é–¢æ•°
fn flag_push(kind: TokenKind, result: &mut Vec<Token>, cur: &mut StrCur, line: u32) {
    let tok = Token {
        kind,
        label: String::from(cur.next()),
        josi: None,
        line,
    };
    result.push(tok);   
}
fn flag_push_josi(kind: TokenKind, result: &mut Vec<Token>, cur: &mut StrCur, line: u32) {
    let label = String::from(cur.next());
    let josi_opt = josi_list::read_josi(cur);
    let tok = Token {
        kind,
        label,
        josi: josi_opt,
        line,
    };
    result.push(tok);   
}
// lenæ–‡å­—ã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦è¿½åŠ ã™ã‚‹é–¢æ•°
fn flag_push_n(kind: TokenKind, flag_ch: char, result: &mut Vec<Token>, cur: &mut StrCur, len: usize, line: u32) {
    let tok = Token {
        kind,
        label: String::from(flag_ch),
        josi: None,
        line,
    };
    cur.seek(len as i32);
    result.push(tok);   
}

fn read_lf(cur: &mut StrCur, line: &mut u32) -> Token {
    let lf = cur.next();
    let t = Token::new_char(TokenKind::Eol, lf, *line);
    *line += 1;
    return t;
}

fn read_linecomment(cur: &mut StrCur, line: &mut u32) -> Token {
    cur.seek(1); // skip "â€»"
    let rem = cur.get_token_tostr('\n');
    let tok = Token::new_str(TokenKind::Comment, &rem, *line);
    *line += 1;
    return tok;
}

fn read_slash(cur: &mut StrCur, line: &mut u32) -> Token {
    // line comment
    if cur.eq_str("//") {
        cur.seek(2); // skip "//"
        let rem = cur.get_token_tostr('\n');
        let tok = Token::new_str(TokenKind::Comment, &rem, *line);
        *line += 1;
        return tok;
    }
    // range comment
    if cur.eq_str("/*") {
        cur.seek(2); // skio "/*"
        let rem = cur.get_token_str("*/");
        let mut ret_cnt = 0;
        for c in rem.iter() {
            if *c == '\n' { ret_cnt += 1; }
        }
        let rem_s: String = rem.iter().collect();
        let tok = Token::new_str(TokenKind::Comment, &rem_s, *line);
        *line += ret_cnt;
        return tok;
    }
    // flag
    let flag = cur.next();
    return Token::new_char(TokenKind::Div, flag, *line);
}

fn read_number(cur: &mut StrCur, line: &mut u32) -> Token {
    let mut vc: Vec<char> = vec![];
    while cur.peek_in_range('0', '9') {
        vc.push(cur.next());
    }
    if cur.peek() == '.' {
        vc.push(cur.next());
        while cur.peek_in_range('0', '9') {
            vc.push(cur.next());
        }
        let num_s: String = vc.iter().collect();
        let josi_opt = josi_list::read_josi(cur);
        return Token::new(TokenKind::Number, num_s, josi_opt, *line);
    }
    let num_s: String = vc.iter().collect();
    let josi_opt = josi_list::read_josi(cur);
    return Token::new(TokenKind::Int, num_s, josi_opt, *line);
}

fn check_special(result: &mut Vec<Token>, cur: &mut StrCur, word: &str, kind: TokenKind, reg_word: &str, line: u32) -> bool {
    if cur.eq_str(word) {
        let len = word.chars().count();
        cur.seek(len as i32);
        let tok = Token::new_str(kind, reg_word, line);
        result.push(tok);
        return true;
    }
    false
}

fn read_word(result: &mut Vec<Token>, cur: &mut StrCur, line: &mut u32) -> bool {
    let mut word: Vec<char> = vec![];
    let mut josi_opt:Option<String> = None;

    // ç‰¹åˆ¥ãªèªå¥ã‚’ä¾‹å¤–ã§ç™»éŒ²ã™ã‚‹
    if cur.eq_str("ã“ã“") {        
        if check_special(result, cur, "ã“ã“ã‹ã‚‰", TokenKind::BlockBegin, "ã“ã“ã‹ã‚‰", *line) { return true; }
        if check_special(result, cur, "ã“ã“ã¾ã§", TokenKind::BlockEnd, "ã“ã“ã¾ã§", *line) { return true; }
    }
    if cur.eq_str("é•") {
        if check_special(result, cur, "é•ãˆã°", TokenKind::Else, "é•", *line) { return true; }
        if check_special(result, cur, "é•ã†ãªã‚‰", TokenKind::Else, "é•", *line) { return true; }
    }
    if check_special(result, cur, "ã¾ãŸã¯", TokenKind::Or, "||", *line) { return true; }
    if check_special(result, cur, "ã‚ã‚‹ã„ã¯", TokenKind::Or, "||", *line) { return true; }
    if check_special(result, cur, "ã‹ã¤", TokenKind::And, "&&", *line) { return true; }
    if check_special(result, cur, "ã‚‚ã—ã‚‚", TokenKind::If, "ã‚‚ã—", *line) { return true; }
    if check_special(result, cur, "ã‚‚ã—", TokenKind::If, "ã‚‚ã—", *line) { return true; }

    // ã²ã‚‰ãŒãªã‚¹ã‚¿ãƒ¼ãƒˆãªã‚‰1æ–‡å­—ç›®ã¯åŠ©è©ã«ãªã‚‰ãªã„
    if kanautils::is_hiragana(cur.peek()) {
        word.push(cur.next());
    }
    
    while cur.can_read() {
        let c = cur.peek();
        // åŠ©è©ã‹ï¼Ÿ
        if kanautils::is_hiragana(c) {
            josi_opt = josi_list::read_josi(cur);
            match josi_opt {
                Some(_) => break, // åŠ©è©ãªã‚‰ç¹°ã‚Šè¿”ã—ã‚’æŠœã‘ã‚‹
                None => {}, // pass
            }
        }
        // wordã«ãªã‚Šå¾—ã‚‹æ–‡å­—ã‹ï¼Ÿ
        if kanautils::is_word_chars(c) {
            word.push(cur.next());
            continue;
        }
        break;
    }
    
    // æœ«å°¾ãŒã€Œå›ã€ãªã‚‰åˆ†å‰²ã€Nå›â†’N|å›
    let has_kai = if word.last() == Some(&'å›') {
        word.pop(); // å›ã‚’å‰Šé™¤
        true
    } else { false };
    // é€ã‚ŠãŒãªã‚’ã‚«ãƒƒãƒˆ
    word = delete_okurigana(word);
    if word.len() > 0 {
        // ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
        let word_s: String = word.iter().collect();
        let kind = reserved_words::check_kind(&word_s);
        let tok = Token::new(kind, word_s, josi_opt, *line);
        result.push(tok);
    }
    //ã€€å›ã‚’è¿½åŠ 
    if has_kai {
        let kai_tok = Token::new(TokenKind::Kai, String::from("å›"), None, *line);
        result.push(kai_tok);
    }
    true
}

fn delete_okurigana(word: Vec<char>) -> Vec<char> {
    // 1æ–‡å­—ãªã‚‰é€ã‚ŠãŒãªã¯ãªã„
    if word.len() <= 1 {
        return word;
    }
    // (ex) ç½®ãæ›ãˆã‚‹ â†’ ç½®æ› ... é€ã‚ŠãŒãªã¯æ¼¢å­—ã‚’æŒŸã‚“ã§ã‚‚å‰Šã‚‹
    // (ex) ãŠå…„ã•ã‚“ â†’ ãŠå…„ ... æ¼¢å­—ã®å¾Œã‚ã®ã²ã‚‰ãŒãªã®ã¿å‰Šã‚‹
    // (ex) ã†ãŸã† â†’ ã†ãŸã† ... å…¨éƒ¨ã²ã‚‰ãŒãªã§ã‚ã‚Œã°å‰Šã‚‰ãªã„
    // (ex) INTã™ã‚‹ â†’ INT ... ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã‚‚æ¼¢å­—ã¨è¦‹ãªã™
    let mut result: Vec<char> = vec![];
    let mut is_hajime_hiragana = true;
    for c in word.iter() {
        // æ¼¢å­—?
        if !kanautils::is_hiragana(*c) {
            is_hajime_hiragana = false;
            result.push(*c);
            continue;
        }
        // å†’é ­ã®ã²ã‚‰ãŒãªã¯è¿½åŠ ã—ç¶šã‘ã‚‹
        if is_hajime_hiragana {
            result.push(*c);
            continue;
        }
    }
    result
}

fn read_string(result: &mut Vec<Token>, cur: &mut StrCur, line: &mut u32, end_flag: char, ex_str: bool) {
    cur.next(); // begin_flag
    let mut res: Vec<char> = vec![];
    let line_begin = *line;
    while cur.can_read() {
        let c = cur.next();
        if c == end_flag {
            break;
        }
        if c == '\n' {
            *line += 1;
        }
        res.push(c);
    }
    // read josi
    let josi_opt = josi_list::read_josi(cur);
    let label = res.iter().collect();
    if ex_str {
        extract_string_ex(result, label, josi_opt, line_begin);
    } else {
        let tok = Token::new(TokenKind::String, label, josi_opt, line_begin);
        result.push(tok);
    }
}

fn extract_string_ex(result: &mut Vec<Token>, src: String, josi_opt:Option<String>, line: u32) {
    let mut data = String::new();
    let mut code = String::new();
    let mut is_extract = false;
    for c in src.chars() {
        if is_extract {
            if c == '}' || c == 'ï½' {
                let list = tokenize_src(&code, line);
                if list.len() > 0 {
                    result.push(Token::new(TokenKind::PlusStr, String::from("çµ"), None, line));
                    result.push(Token::new(TokenKind::ParenL, String::from("("), None, line));
                    for t in list.into_iter() {
                        result.push(t);
                    }
                    result.push(Token::new(TokenKind::ParenR, String::from(")"), None, line));
                    result.push(Token::new(TokenKind::PlusStr, String::from("çµ"), None, line));
                    is_extract = false;
                }
                continue;
            }
            code.push(c);
            continue;
        }
        if c == '{' || c == 'ï½›' {
            is_extract = true;
            result.push(Token::new(TokenKind::String, data, None, line));
            data = String::new();
            continue;
        }
        data.push(c);
    }
    result.push(Token::new(TokenKind::String, data, josi_opt.clone(), line));
}



#[cfg(test)]
mod test_tokenizer {
    use super::*;

    fn delete_okurigana_str(word: &str) -> String {
        let word_v:Vec<char> = word.chars().collect();
        let res_v = delete_okurigana(word_v);
        res_v.iter().collect()
    }
    
    #[test]
    fn test_tokenize() {
        let t = tokenize("//abc");
        assert_eq!(tokens_string(&t), "[Comment:abc]");
        let t = tokenize("//abc\n\n/*ABC*/");
        assert_eq!(tokens_string(&t), "[Comment:abc][Eol][Comment:ABC]");
        let t = tokenize("3\n3.14");
        assert_eq!(tokens_string(&t), "[Int:3][Eol][Number:3.14]");
        let t = tokenize("hoge=35");
        assert_eq!(tokens_string(&t), "[Word:hoge][=][Int:35]");
        let t = tokenize("å¹´é½¢=15");
        assert_eq!(tokens_string(&t), "[Word:å¹´é½¢][=][Int:15]");
        let t = tokenize("(3.0)");
        assert_eq!(tokens_string(&t), "[(][Number:3.0][)]");
        let t = tokenize("A=3*5");
        assert_eq!(tokens_string(&t), "[Word:A][=][Int:3][*][Int:5]");
    }
    #[test]
    fn test_tokenize_josi() {
        let t = tokenize("Aã‹ã‚‰Bã¾ã§");
        assert_eq!(tokens_string(&t), "[Word:A/ã‹ã‚‰][Word:B/ã¾ã§]");
        let t = tokenize("çŠ¬ã‚’ãƒã‚³ã¸");
        assert_eq!(tokens_string(&t), "[Word:çŠ¬/ã‚’][Word:ãƒã‚³/ã¸]");
    }
    #[test]
    fn test_tokenize_str() {
        let t = tokenize("35ã‹ã‚‰ã€Œabcã€ã¾ã§ç½®æ›");
        assert_eq!(tokens_string(&t), "[Int:35/ã‹ã‚‰][String:abc/ã¾ã§][Word:ç½®æ›]");
        let t = tokenize("ã€Œï¼‘ï¼’ï¼“123ã€");
        assert_eq!(tokens_string(&t), "[String:ï¼‘ï¼’ï¼“123]");
        let t = tokenize("'hoge'");
        assert_eq!(tokens_string(&t), "[String:hoge]");
        let t = tokenize("ã€booã€");
        assert_eq!(tokens_string(&t), "[String:boo]");
    }

    #[test]
    fn test_delete_okurigana() {
        assert_eq!(delete_okurigana_str("åˆ‡å–ã‚Š"), String::from("åˆ‡å–"));
        assert_eq!(delete_okurigana_str("ç½®ãæ›ãˆã‚‹"), String::from("ç½®æ›"));
        assert_eq!(delete_okurigana_str("ãªã§ã—ã“"), String::from("ãªã§ã—ã“"));
        assert_eq!(delete_okurigana_str("ãŠå…„ã¡ã‚ƒã‚“"), String::from("ãŠå…„"));
        assert_eq!(delete_okurigana_str("Fä¾¡æ ¼"), String::from("Fä¾¡æ ¼"));
        assert_eq!(delete_okurigana_str("VSé£Ÿã¹ã‚‹"), String::from("VSé£Ÿ"));
        assert_eq!(delete_okurigana_str("INTã™ã‚‹"), String::from("INT"));
    }

    #[test]
    fn test_reserved_word() {
        let t = tokenize("35å›");
        assert_eq!(tokens_string(&t), "[Int:35][Kai]");
        let t = tokenize("Nå›");
        assert_eq!(tokens_string(&t), "[Word:N][Kai]");
    }

    #[test]
    fn test_word_check() {
        let t = tokenize("35å›ã€ãƒ¯ãƒ³ã€ã¨è¡¨ç¤º");
        assert_eq!(tokens_string(&t), "[Int:35][Kai][String:ãƒ¯ãƒ³/ã¨][Word:è¡¨ç¤º]");
        let t = tokenize("Nå›");
        assert_eq!(tokens_string(&t), "[Word:N][Kai]");
    }

    #[test]
    fn test_extract_string() {
        let t = tokenize("ã€Œa={a}ã€ã¨è¡¨ç¤º");
        assert_eq!(tokens_string(&t), "[String:a=][&][(][Word:a][)][&][String:/ã¨][Word:è¡¨ç¤º]");
    }
}
